{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72db5616-d5e3-4d33-ab5f-3797adaf067c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Intro\n",
    "\n",
    "The idea of this notebook was to figure out, how to use the potential of the cluster and parallelize the training of different ML models by using spark and hyperopt. Hyperopt is a general-purpose library, which can optimize any function that has parameters here is will use it for the loss function of the prediction (rmse).\n",
    "\n",
    "I understood that in the end a monthly aggredation is needed but i will work with daily data since the effect of weekdays and holidays is more visible. \n",
    "\n",
    "Simon B, 05.06.2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4312f479-612c-493c-938f-cdbf1b2f42e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install prophet==1.1.4 hyperopt==0.2.7 sklearn mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d12ee0-77cf-487b-af80-f72b0dab99d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6242e95b-04ca-447e-a6b8-8a8dbb8dbff5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics         import mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from hyperopt import fmin, hp, tpe\n",
    "from hyperopt import SparkTrials, STATUS_OK\n",
    "from prophet import Prophet\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "933189d5-41ff-4e7a-b3f3-fa07fa6455df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Get Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "021f74db-1b3c-42fa-9a3a-c99c7817b09e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the name of the experiment \n",
    "# You must create the expirement by hand in the Databricks Machine Learning Experiments GUI\n",
    "# Then copy the name of the experiment and paste it here\n",
    "\n",
    "experimentPath = \"/Users/simon.buse@ewz.ch/MittelfristPrognoseTest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a9e4dd7-0615-4587-b2a6-5bbd830b1cdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def logMetrics(test, predictions):\n",
    "  '''\n",
    "  Simple function to compute MAPE and RMSE metrics from test predictions and the test data for our models.\n",
    "  The error metrics will be logged with MLflow.\n",
    "  '''\n",
    "  mapeValue = mean_absolute_percentage_error(test, predictions)\n",
    "  rmseValue = mean_squared_error(test, predictions, squared=False)\n",
    "  metrics = {\"mape\": mapeValue, \"rmse\": rmseValue}\n",
    "\n",
    "  # MLflow will log the metrics to the currently active run\n",
    "  mlflow.log_metrics(metrics)\n",
    "\n",
    "\n",
    "def logParams(parameters):\n",
    "  '''\n",
    "  Parameters should be a dictionary with the form {\"parameterName\": \"parameterValue\"}\n",
    "  '''\n",
    "  for par in parameters:\n",
    "    mlflow.log_params(par)\n",
    "\n",
    "def reassign_outliers(pdf):\n",
    "  \"\"\"There is an extrem outlier in the data which is probably a mistake. I will reassign the value to it's neighbour.\"\"\"\n",
    "\n",
    "  for column in pdf.columns:\n",
    "    \n",
    "    outlier_loc = np.where(pdf[column] < np.mean(pdf[column])-3*np.std(pdf[column]))\n",
    "    (pdf[column].values)[outlier_loc] = np.mean(pdf[column]) \n",
    "\n",
    "    print(f\"Reassigned {len(outlier_loc)} values in the column {column}. These values where more than 3 sigma away from the mean.\")\n",
    "\n",
    "  return pdf\n",
    "\n",
    "\n",
    "def resample_fix_ends(pdf,frequency):\n",
    "  \"\"\"\n",
    "  The function resamples the data according to the sampling frequency. \n",
    "  Often the first and the last data-point are deviating a lot from the rest of the series.\n",
    "  \n",
    "  As a simple fix i will just delete the first and the last value if they deviate more than 20% to their neighbour. \n",
    "  \"\"\"\n",
    "\n",
    "  pdf = pdf.resample(frequency).sum(min_count=1) #\"D,W,M\"\n",
    "\n",
    "  for column in pdf.columns:\n",
    "    if pdf[column].iloc[0] < 0.8*pdf[column].iloc[1]:\n",
    "      pdf = pdf.drop(pdf.index[0]) \n",
    "      #pdf.at[pdf.index[0],column]   = pdf[column].iloc[1] #this would assigne the value of the next day to the first day.\n",
    "\n",
    "    if pdf[column].iloc[-1] < 0.8*pdf[column].iloc[-2]:\n",
    "      pdf = pdf.drop(pdf.index[-1]) \n",
    "      #pdf.at[pdf.index[-1],column]  = pdf[column].iloc[-2] #this would assigne the value of the second last day to the last day.\n",
    "\n",
    "  return pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2943c65c-5e8b-4b62-ae40-822de0d4f846",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "490fbd03-8340-4084-acad-5be0bd97a709",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://data.stadt-zuerich.ch/dataset/ewz_stromabgabe_netzebenen_stadt_zuerich/download/ewz_stromabgabe_netzebenen_stadt_zuerich.csv\"\n",
    "pdf = pd.read_csv(url,index_col=None)\n",
    "\n",
    "pdf[\"Timestamp\"] =  pd.to_datetime(pdf['Timestamp'],utc=True)\n",
    "\n",
    "pdf = pdf.set_index(pdf[\"Timestamp\"])\n",
    "pdf = resample_fix_ends(pdf,\"D\")\n",
    "pdf = reassign_outliers(pdf)\n",
    "\n",
    "pdf.index = pdf.index.tz_localize(None)  #Let's drop the timezone info to avoid warnings\n",
    "pdf[\"ds\"]= pdf.index\n",
    "\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7668b8e9-828a-4a8e-afde-8355f01bc65e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adding corona as one-off holidays https://facebook.github.io/prophet/docs/handling_shocks.html\n",
    "corona = pd.DataFrame({\n",
    "  'holiday': 'corona',\n",
    "  'ds': pd.date_range(start='2020-03-01', end='2020-12-31', freq='D'),\n",
    "  'lower_window': 0,\n",
    "  'upper_window': (pd.to_datetime('2020-12-31') - pd.to_datetime('2020-03-01')).days,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbeefc6b-a5f9-454e-ae9b-8afc64d790d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define the Model and the Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea3ea14f-64b2-4a1d-a527-c1aeae08cd66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_prophet(pdfTrain,pdfTest,tracked_hyperparams, maxEvals=10,timeoutSec=5*60):\n",
    "  \"\"\"This function is just a wrapper for the hyperopt procedure.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "  def train(params):\n",
    "    \"\"\"\n",
    "    This is our main training function which we pass to Hyperopt.\n",
    "    It takes in hyperparameter settings, fits a model based on those settings,\n",
    "    evaluates the model, and returns the loss. \n",
    "    \"\"\"\n",
    "\n",
    "    if tracked_hyperparams[\"corona\"] == True:\n",
    "      forecaster = Prophet(\n",
    "        holidays                = corona,\n",
    "        seasonality_mode        = tracked_hyperparams['seasonality_mode'],\n",
    "        changepoint_prior_scale = params['changepoint_prior_scale'],\n",
    "        seasonality_prior_scale = params['seasonality_prior_scale'],\n",
    "        holidays_prior_scale    = params['holidays_prior_scale'],\n",
    "        changepoint_range       = params['changepoint_range'])\n",
    "      \n",
    "    else:\n",
    "      forecaster = Prophet(\n",
    "        seasonality_mode        = tracked_hyperparams['seasonality_mode'],\n",
    "        changepoint_prior_scale = params['changepoint_prior_scale'],\n",
    "        seasonality_prior_scale = params['seasonality_prior_scale'],\n",
    "        holidays_prior_scale    = params['holidays_prior_scale'],\n",
    "        changepoint_range       = params['changepoint_range'])\n",
    "\n",
    "    if tracked_hyperparams[\"holidays\"] != None:   \n",
    "      forecaster.add_country_holidays(country_name=tracked_hyperparams[\"holidays\"])\n",
    "\n",
    "    forecaster.fit(pdfTrain)  \n",
    "    y_pred = forecaster.predict(pdfTest)\n",
    "\n",
    "    rmse   = mean_squared_error(y_true=pdfTest.y.values, y_pred=y_pred.yhat.values, squared=False)  \n",
    "\n",
    "    return {'loss': rmse, 'status': STATUS_OK, 'Trained_Model': forecaster}\n",
    "   \n",
    "  # Define the search space for Hyperopt.Prophets main parameter where found here\n",
    "  # https://facebook.github.io/prophet/docs/diagnostics.html#hyperparameter-tuning \n",
    "\n",
    "  search_space = {\n",
    "    \"changepoint_prior_scale\": hp.loguniform(\"changepoint_prior_scale\", -6.9, -0.69), #equivalent to [0.001,0.5]\n",
    "    \"seasonality_prior_scale\": hp.loguniform(\"seasonality_prior_scale\", -6.9, 2.3),   #equivalent to [0.001, 10]\n",
    "    \"holidays_prior_scale\":    hp.loguniform(\"holidays_prior_scale\", -6.9, 2.3),      #equivalent to [0.001, 10]\n",
    "    \"changepoint_range\":       hp.uniform(\"changepoint_range\", 0.8,0.95)              #optional according to docs, default = 0.8               \n",
    "  }\n",
    "\n",
    "  # Select a search algorithm for Hyperopt to use.\n",
    "  algo=tpe.suggest  # Tree of Parzen Estimators, a Bayesian method\n",
    "\n",
    "  # Distribute tuning across our Spark cluster\n",
    "  spark_trials = SparkTrials(parallelism=4)\n",
    "\n",
    "  best_hyperparameters = fmin(fn=train,space=search_space,algo=algo,trials=spark_trials,max_evals=maxEvals,timeout=timeoutSec)\n",
    "  best_model = spark_trials.results[np.argmin([r['loss'] for r in spark_trials.results])]['Trained_Model']\n",
    "\n",
    "  print(best_hyperparameters)\n",
    "  return best_model,best_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ccbe35-ac30-4c73-b412-b566bc454caf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Train a single model on the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54774901-b8c3-4907-8141-cd80890f5625",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#train singel model on the sum of the two columns, let's add the tracking with Mlflow and comparison\n",
    "\n",
    "sum_pdf = pdf.copy()\n",
    "sum_pdf[\"total\"] = pdf[\"Value_NE5\"].values + pdf[\"Value_NE7\"].values\n",
    "sum_pdf = sum_pdf.drop(columns=[\"Value_NE5\",\"Value_NE7\"])\n",
    "sum_pdf = sum_pdf.rename(columns={\"total\": \"y\"})\n",
    "split = int(len(sum_pdf)*0.9)\n",
    "pdf_train, pdf_test = sum_pdf.iloc[:split], sum_pdf.iloc[split:]\n",
    "\n",
    "\n",
    "#define the hyperparams to track: holidays, additive or multiplicative, adding corona by hand or not, \n",
    "tracked_hyperparams =   {\"seasonality_mode\":['multiplicative','additive'], \n",
    "                         \"corona\":          [False, True],\n",
    "                         \"holidays\":        [None, 'Switzerland']}\n",
    "\n",
    "grid = ParameterGrid(tracked_hyperparams)\n",
    "\n",
    "for evaluation_params in grid:\n",
    "  \n",
    "  # Create an MLflow run for each point in the grid space. \n",
    "  # The \"with\" syntax means the run will be closed once the following block of code is finished.\n",
    "  with mlflow.start_run(run_name = \"Prophet Model\") as run:\n",
    "    \n",
    "    # Set a tag with the model type\n",
    "    mlflow.set_tags({\"Model\": \"Prophet\"})\n",
    "\n",
    "    #create, train and return model\n",
    "    print(evaluation_params)\n",
    "    best_model,best_hyperparameters = train_prophet(pdf_train,pdf_test, evaluation_params)\n",
    "\n",
    "    prediction = best_model.predict(pdf_test) \n",
    "\n",
    "    # Log the metrics in MLflow \n",
    "    logMetrics(pdf_test.y.values, prediction.yhat.values)\n",
    "\n",
    "    # Log hyperparameters\n",
    "    logParams([best_hyperparameters,evaluation_params])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad329c7-55e2-4b1b-865a-9e5f8d528ad2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#plot the outcome of the model on the summed data.\n",
    "\n",
    "f, axes = plt.subplots(2, 1,figsize=(18,8))\n",
    "\n",
    "y_pred = best_model.predict(pdf_test)\n",
    "\n",
    "axes[0].plot(y_pred.ds.values, y_pred.yhat.values, color=\"tab:red\", label=\"forcast\")\n",
    "#axes[0].plot(pdf_train.ds.values, pdf_train.y.values, color=\"tab:blue\", label=\"train\")\n",
    "axes[0].plot(pdf_test.ds.values, pdf_test.y.values, color=\"tab:orange\", label=\"truth\", alpha=0.5)\n",
    "axes[0].legend()\n",
    "axes[0].set_title(\"NE5 + NE7\")\n",
    "axes[0].set_ylabel(\"Last [kWh]\")\n",
    "\n",
    "\n",
    "xmin, xmax = axes[0].get_xlim()\n",
    "\n",
    "axes[1].plot(pdf_test.ds,(pdf_test.y.values-y_pred.yhat.values)/(pdf_test.y.values)*100)\n",
    "axes[1].set_xlim(xmin, xmax)\n",
    "axes[1].set_ylabel(\"residual: True-Pred/True [%]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89611333-f1fe-47d0-93b8-1cf6ef057785",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast = best_model.predict(pdf)\n",
    "best_model.plot(forecast)\n",
    "fig = best_model.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3946216-c68d-49c2-a1ff-75b4087423a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Train 2 models for NE5 and NE7 separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d307c9-f809-4149-908f-4b9e36a5eb94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#train two separate models for the two series separately\n",
    "best_induvidual_models = {}\n",
    "best_model_params = {}\n",
    "\n",
    "split = int(len(pdf)*0.9)\n",
    "colum_names = [\"Value_NE5\",\"Value_NE7\"]\n",
    "\n",
    "for colum in colum_names:\n",
    "  \n",
    "  induvidual_pdf = pdf.copy()\n",
    "\n",
    "  colum_to_drop = [x for x in colum_names if x != colum]\n",
    "  induvidual_pdf = induvidual_pdf.drop(columns=colum_to_drop) #drop the unwanted columns\n",
    "  induvidual_pdf = induvidual_pdf.rename(columns={colum: \"y\"})\n",
    "  \n",
    "  pdf_train, pdf_test = induvidual_pdf.iloc[:split], induvidual_pdf.iloc[split:]\n",
    "  best_induvidual_models[colum],best_model_params[colum] = train_prophet(pdf_train,pdf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04e8ab5b-6efd-4341-9b18-6c9d7865af3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#plot the outcome of two separate models\n",
    "\n",
    "induvidual_predictions = []\n",
    "\n",
    "for i,colum in enumerate(colum_names):\n",
    "\n",
    "  pdf1 = pdf.copy()\n",
    "  colum_to_drop = [x for x in colum_names if x != colum]\n",
    "  pdf1 = pdf1.drop(columns=colum_to_drop)\n",
    "  pdf1 = pdf1.rename(columns={colum: \"y\"})\n",
    "\n",
    "  best_induvidual = best_induvidual_models[colum]\n",
    "\n",
    "  pdf1_train, pdf1_test = pdf1.iloc[:split], pdf1.iloc[split:]\n",
    "  y_pred = best_induvidual.predict(pdf1_test)\n",
    "  \n",
    "  f, axes = plt.subplots(2, 1,figsize=(18,8))\n",
    "\n",
    "  axes[0].set_title(colum)\n",
    "  axes[0].plot(y_pred.ds.values, y_pred.yhat.values, color=\"tab:red\", label=\"forcast\")\n",
    "#  axes[0].plot(pdf1_train.ds.values, pdf1_train.y.values, color=\"tab:blue\", label=\"train\")\n",
    "  axes[0].plot(pdf1_test.ds.values, pdf1_test.y.values, color=\"tab:orange\", label=\"truth\", alpha=0.5)\n",
    "  axes[0].legend()\n",
    "\n",
    "\n",
    "  xmin, xmax = axes[0].get_xlim()\n",
    "  \n",
    "  axes[1].plot(pdf1_test.ds,(pdf1_test.y.values-y_pred.yhat.values)/(pdf1_test.y.values)*100)\n",
    "  axes[1].set_xlim(xmin, xmax)\n",
    "  axes[1].set_ylabel(\"residual True-Pred/True [%]\")\n",
    "\n",
    "  \n",
    "  plt.show()\n",
    "\n",
    "  induvidual_predictions.append([y_pred.yhat.values,pdf1_test.y.values])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5bced0a-8c19-4816-8292-c2b653da37e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment for a plot with components\n",
    "\n",
    "# for i,colum in enumerate(colum_names):\n",
    "\n",
    "#   pdf1 = pdf.copy()\n",
    "#   colum_to_drop = [x for x in colum_names if x != colum]\n",
    "#   pdf1 = pdf1.drop(columns=colum_to_drop)\n",
    "#   pdf1 = pdf1.rename(columns={colum: \"y\"})\n",
    "\n",
    "#   pdf1_train, pdf1_test = pdf1.iloc[:split], pdf1.iloc[split:]\n",
    "#   best_induvidual = best_induvidual_models[colum]\n",
    "\n",
    "#   forecast = best_induvidual.predict(pdf1)\n",
    "#   print(colum)\n",
    "\n",
    "#   best_induvidual.plot(forecast)\n",
    "#   fig = best_induvidual.plot_components(forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4d87888-ba49-4185-b673-595f4971f05f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "induvidual_predictions = np.array(induvidual_predictions)\n",
    "prediction_on_sum = np.sum(induvidual_predictions,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be0bec16-2384-4b3e-b3dd-f8b937d74f74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"The rmse of the two separate models on the total is: {:.2e} \".format(mean_squared_error(y_true=prediction_on_sum[1], y_pred=prediction_on_sum[0], square_root=True)))\n",
    "print(\"The rmse of the one induvidual model is:             {:.2e}\".format(mean_squared_error(y_true=pdf_test.y.values, y_pred=y_pred.yhat.values, square_root=True)))\n",
    "\n",
    "# It looks like one model is sufficient and we can simply operate on the sum of NE5 and NE7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc20048d-c079-4ef4-9e52-2958e870e7a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "759fc860-d780-4fe0-97c6-2c530d131ebb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# resample to monthly if necessary\n",
    "\n",
    "# f, axes = plt.subplots(1, 1,figsize=(18,8))\n",
    "\n",
    "# y_pred    = resample_fix_ends(induvidual_predictions[1][0],\"M\")+resample_fix_ends(induvidual_predictions[0][0],\"M\").values\n",
    "# pdf_train = resample_fix_ends(induvidual_predictions[1][1],\"M\")+resample_fix_ends(induvidual_predictions[0][1],\"M\").values         \n",
    "# pdf_test  = resample_fix_ends(induvidual_predictions[1][2],\"M\")+resample_fix_ends(induvidual_predictions[0][2],\"M\").values         \n",
    "\n",
    "# plt.plot(y_pred, color=\"tab:red\",label=\"forcast\")\n",
    "# plt.plot(pdf_train, color=\"tab:green\",label=\"train\")\n",
    "# plt.plot(pdf_test, color=\"tab:orange\", label=\"test\")\n",
    "# plt.title(\"Sum of NE5+NE7\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# #evaluate how good the model is--> rmse\n",
    "# rmse   = mean_squared_error(y_true=pdf_test, y_pred=y_pred, square_root=True) \n",
    "# print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a78488a-40e3-4b89-b502-b41a74397ad6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# space = hp.choice('classifier_type', [\n",
    "#     {\n",
    "#         'type': 'naive_bayes',\n",
    "#     },\n",
    "#     {\n",
    "#         'type': 'svm',\n",
    "#         'C': hp.lognormal('svm_C', 0, 1),\n",
    "#         'kernel': hp.choice('svm_kernel', [\n",
    "#             {'ktype': 'linear'},\n",
    "#             {'ktype': 'RBF', 'width': hp.lognormal('svm_rbf_width', 0, 1)},\n",
    "#             ]),\n",
    "#     },\n",
    "#     {\n",
    "#         'type': 'dtree',\n",
    "#         'criterion': hp.choice('dtree_criterion', ['gini', 'entropy']),\n",
    "#         'max_depth': hp.choice('dtree_max_depth',\n",
    "#             [None, hp.qlognormal('dtree_max_depth_int', 3, 1, 1)]),\n",
    "#         'min_samples_split': hp.qlognormal('dtree_min_samples_split', 2, 1, 1),\n",
    "#     },\n",
    "#     ])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "hyperoptProphet",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
